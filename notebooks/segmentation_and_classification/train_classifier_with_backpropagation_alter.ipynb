{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flim.experiments import utils, LIDSDataset, ToTensor\n",
    "from flim.models.lcn import LCNCreator\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchsummary import summary\n",
    "import torchmetrics as tm\n",
    "\n",
    "from math import ceil, floor\n",
    "\n",
    "import copy\n",
    "\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a GPU/CPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the folder which contains your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = path.join(\"..\", \"data\", \"corel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a feature extractor using FLIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load architecture\n",
    "# in this example, the architecure specifies a feature extractor and a classifier\n",
    "architecture = utils.load_architecture(path.join(base_dir, 'archs', 'arch-with-classifier.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images and markers\n",
    "images, markers = utils.load_images_and_markers(path.join(base_dir, 'images-and-markers'))\n",
    "\n",
    "# get input shape\n",
    "ncols     = images.shape[1]\n",
    "nrows     = images.shape[2]\n",
    "nchannels = images.shape[3]\n",
    "nclasses  = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building conv1\n",
      "Building activ1\n",
      "Building pool1\n",
      "Building conv2\n",
      "Building activ2\n",
      "Building pool2\n",
      "Building linear1\n",
      "Building activ3\n",
      "Building drop1\n",
      "Building linear2\n"
     ]
    }
   ],
   "source": [
    "# build model and learn convolutional layers with FLIM\n",
    "creator = LCNCreator(architecture, images=images, markers=markers, relabel_markers=True, device=device)\n",
    "creator.build_model()\n",
    "\n",
    "model = creator.get_LIDSConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feeze feature extractor\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1                          []                        --\n",
      "|    └─Conv2d: 2-1                       [-1, 32, 400, 400]        (2,400)\n",
      "|    └─ReLU: 2-2                         [-1, 32, 400, 400]        --\n",
      "|    └─MaxPool2d: 2-3                    [-1, 32, 200, 200]        --\n",
      "|    └─Conv2d: 2-4                       [-1, 64, 200, 200]        (18,432)\n",
      "|    └─ReLU: 2-5                         [-1, 64, 200, 200]        --\n",
      "|    └─MaxPool2d: 2-6                    [-1, 64, 100, 100]        --\n",
      "├─Sequential: 1                          []                        --\n",
      "|    └─Flatten: 2-7                      [-1, 640000]              --\n",
      "|    └─Linear: 2-8                       [-1, 512]                 327,680,512\n",
      "|    └─ReLU: 2-9                         [-1, 512]                 --\n",
      "|    └─Dropout: 2-10                     [-1, 512]                 --\n",
      "|    └─Linear: 2-11                      [-1, 6]                   3,078\n",
      "==========================================================================================\n",
      "Total params: 327,704,422\n",
      "Trainable params: 327,683,590\n",
      "Non-trainable params: 20,832\n",
      "Total mult-adds (G): 1.45\n",
      "==========================================================================================\n",
      "Input size (MB): 1.83\n",
      "Forward/backward pass size (MB): 58.60\n",
      "Params size (MB): 1250.09\n",
      "Estimated Total Size (MB): 1310.52\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1                          []                        --\n",
       "|    └─Conv2d: 2-1                       [-1, 32, 400, 400]        (2,400)\n",
       "|    └─ReLU: 2-2                         [-1, 32, 400, 400]        --\n",
       "|    └─MaxPool2d: 2-3                    [-1, 32, 200, 200]        --\n",
       "|    └─Conv2d: 2-4                       [-1, 64, 200, 200]        (18,432)\n",
       "|    └─ReLU: 2-5                         [-1, 64, 200, 200]        --\n",
       "|    └─MaxPool2d: 2-6                    [-1, 64, 100, 100]        --\n",
       "├─Sequential: 1                          []                        --\n",
       "|    └─Flatten: 2-7                      [-1, 640000]              --\n",
       "|    └─Linear: 2-8                       [-1, 512]                 327,680,512\n",
       "|    └─ReLU: 2-9                         [-1, 512]                 --\n",
       "|    └─Dropout: 2-10                     [-1, 512]                 --\n",
       "|    └─Linear: 2-11                      [-1, 6]                   3,078\n",
       "==========================================================================================\n",
       "Total params: 327,704,422\n",
       "Trainable params: 327,683,590\n",
       "Non-trainable params: 20,832\n",
       "Total mult-adds (G): 1.45\n",
       "==========================================================================================\n",
       "Input size (MB): 1.83\n",
       "Forward/backward pass size (MB): 58.60\n",
       "Params size (MB): 1250.09\n",
       "Estimated Total Size (MB): 1310.52\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (nchannels,nrows,ncols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics() -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dictionary of metrics to compute.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"acc\": tm.Accuracy().to(device),\n",
    "        \"kappa\": tm.CohenKappa(num_classes=6).to(device),\n",
    "    }\n",
    "\n",
    "def training_step(model: nn.Module,\n",
    "                  batch_x: Tensor,\n",
    "                  batch_y: Tensor,\n",
    "                  optimizer: optim.Optimizer,\n",
    "                  metrics: dict) -> Tensor:\n",
    "    \"\"\"\n",
    "    One training step over one batch (batch_x, batch_y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model   : nn.Module\n",
    "        Model to train.\n",
    "    batch_x : torch.Tensor\n",
    "        Input batch.\n",
    "    batch_y : torch.Tensor\n",
    "        Target batch.\n",
    "    optimizer : optim.Optimizer\n",
    "        Optimizer to use.\n",
    "    metrics : dict\n",
    "        Metrics to compute.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : Tensor\n",
    "        Loss computed on the batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = model(batch_x)\n",
    "    loss = F.cross_entropy(output, batch_y)\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    for _, metric in metrics.items():\n",
    "        metric(output.softmax(dim=1), batch_y)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def evaluation_step(model: nn.Module, \n",
    "                    batch_x: torch.Tensor,\n",
    "                    batch_y: torch.Tensor,\n",
    "                    metrics: dict) -> Tensor:\n",
    "    \"\"\"\n",
    "    One evaluation step over one batch (batch_x, batch_y).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model   : nn.Module\n",
    "        Model to evaluate.\n",
    "    batch_x : torch.Tensor\n",
    "        Input batch.\n",
    "    batch_y : torch.Tensor\n",
    "        Target batch.\n",
    "    metrics : dict\n",
    "        Metrics to compute.\n",
    "    Returns\n",
    "    -------\n",
    "    loss: Tensor\n",
    "        Loss computed on the batch.\n",
    "    \"\"\"\n",
    "    output = model(batch_x)\n",
    "    loss = F.cross_entropy(output, batch_y)\n",
    "\n",
    "    for _, metric in metrics.items():\n",
    "        metric(output.softmax(dim=1), batch_y)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train set. It should be a folder with images named as 0000{label}_0000{imge_number}.{image_format}\n",
    "# You can create a .txt file with the name of the images in the train set and another file with the name of the images in test set\n",
    "trainset = LIDSDataset(path.join(base_dir, 'dataset'), path.join(base_dir, 'train.txt'), transform=ToTensor())\n",
    "# split train set in train and validation set\n",
    "# consider forcing splits to be stratified\n",
    "trainset, valset = random_split(trainset, [ceil(len(trainset)*0.8), floor(len(trainset)*0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, drop_last=False)\n",
    "valloader = DataLoader(valset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/afalcao/miniconda3/envs/FLIM/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "lr      = 0.00001\n",
    "w_decay = 0.1\n",
    "epochs  = 30\n",
    "\n",
    "# set feature extraction in evaluation mode  \n",
    "model.features.eval()\n",
    "\n",
    "# get the trainable parameters\n",
    "params = [param for param in model.parameters() if param.requires_grad]\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(params, lr=lr, weight_decay=w_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, cooldown=3, factor=0.1, verbose=False)\n",
    "# get evaluation metrics      \n",
    "train_metrics = get_metrics()\n",
    "val_metrics = get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.22 GiB (GPU 0; 10.75 GiB total capacity; 7.50 GiB already allocated; 444.44 MiB free; 8.61 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d2a4369d0aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r[training]\\tepoch: {epoch}/{epochs-1}\\titeration: {i}/{N-1}\\tloss: {loss.item():.6f}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-77fccdb08c24>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(model, batch_x, batch_y, optimizer, metrics)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FLIM/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FLIM/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FLIM/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/FLIM/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.22 GiB (GPU 0; 10.75 GiB total capacity; 7.50 GiB already allocated; 444.44 MiB free; 8.61 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# save the model with the best validation kappa\n",
    "best_model = None\n",
    "best_kappa = -1\n",
    "\n",
    "# training loop\n",
    "for epoch in range(0, epochs):\n",
    "    # training setp\n",
    "    N = len(trainloader)\n",
    "    train_loss = 0 \n",
    "    for i, data in enumerate(trainloader):\n",
    "        model.classifier.train()\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        loss = training_step(model, inputs, labels, optimizer, train_metrics)\n",
    "        train_loss += loss\n",
    "        print(f\"\\r[training]\\tepoch: {epoch}/{epochs-1}\\titeration: {i}/{N-1}\\tloss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "    train_loss = train_loss / N\n",
    "    train_acc = train_metrics['acc'].compute()\n",
    "    train_kappa = train_metrics['kappa'].compute()\n",
    "\n",
    "    print(f\"\\r[training]\\tepoch: {epoch}/{epochs-1}\\tloss: {train_loss.item():.6f}\\tacc: {train_acc:.6f}\\tkappa: {train_kappa:.6f}\")\n",
    "\n",
    "    # validation step\n",
    "    N = len(valloader)\n",
    "    val_loss = 0\n",
    "    for i, data in enumerate(valloader):\n",
    "        with torch.no_grad():\n",
    "            model.classifier.eval()\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            loss = evaluation_step(model, inputs, labels, val_metrics)\n",
    "            val_loss += loss\n",
    "            print(f\"\\r[validating]\\tepoch: {epoch}/{epochs-1}\\titeration: {i}/{N-1}\\tloss: {loss.item():.6f}\", end=\"\")\n",
    "\n",
    "    val_loss = val_loss / N\n",
    "    val_acc = val_metrics[\"acc\"].compute()\n",
    "    val_kappa = val_metrics[\"kappa\"].compute()\n",
    "\n",
    "    print(f\"\\r[validating]\\tepoch: {epoch}/{epochs-1}\\tloss: {val_loss.item():.6f}\\tacc: {val_acc:.6f}\\tkappa: {val_kappa:.6f}\")\n",
    "\n",
    "    # save the model if it has the best validation kappa\n",
    "    if epoch == 0 or val_kappa > best_kappa:\n",
    "        best_kappa = val_kappa\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model weights\n",
    "torch.save(best_model.state_dict(), \"./model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test set. It should be a folder with images named as 0000{label}_0000{imge_number}.{image_format}\n",
    "# You can create a .txt file with the name of the images in the test set\n",
    "testset = LIDSDataset(path.join(base_dir, 'dataset'), path.join(base_dir, 'test.txt'), transform=ToTensor())\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get evalutation metrics\n",
    "test_metrics = get_metrics()\n",
    "\n",
    "# set model in evaluation mode\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# testing loop\n",
    "N = len(testloader)\n",
    "for i, data in enumerate(testloader):\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    loss = evaluation_step(model, inputs, labels, test_metrics)\n",
    "    print(f\"\\r[testing]\\titeration: {i}/{N-1}\\tloss: {loss.item()}\", end=\"\")\n",
    "\n",
    "test_acc = test_metrics[\"acc\"].compute()\n",
    "test_kappa = test_metrics[\"kappa\"].compute()\n",
    "\n",
    "print(f\"\\r[testing] acc: {test_acc:.6f}\\tkappa: {test_kappa:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ddcd6092431b27da76678c78100b28c2416c7a18b759e433950fb611764dce11"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
